var documenterSearchIndex = {"docs":
[{"location":"guide/#User-Guide","page":"User Guide","title":"User Guide","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"This guide covers the stats-based workflow for leak-free label normalization in machine learning.","category":"page"},{"location":"guide/#Critical:-Always-Use-the-Stats-Based-Workflow","page":"User Guide","title":"⚠️ Critical: Always Use the Stats-Based Workflow","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"NEVER use normalize_labels() directly on your full dataset. This causes data leakage! Instead, follow this pattern:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Compute stats from training data ONLY\nApply the same stats to validation/test data\nDenormalize predictions using the same stats","category":"page"},{"location":"guide/#The-Three-Step-Pattern","page":"User Guide","title":"The Three-Step Pattern","text":"","category":"section"},{"location":"guide/#Step-1:-Compute-Normalization-Statistics-(Training-Data-Only)","page":"User Guide","title":"Step 1: Compute Normalization Statistics (Training Data Only)","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"using RealLabelNormalization\n\n# Your training labels (with outliers)\ntrain_labels = [1.2, 5.8, 3.4, 8.1, 2.3, 100.5]  # 100.5 is an outlier\n\n# Compute stats from training data ONLY\nstats = compute_normalization_stats(train_labels; method=:zscore, clip_quantiles=(0.01, 0.99))","category":"page"},{"location":"guide/#Step-2:-Apply-Stats-to-All-Data-Splits","page":"User Guide","title":"Step 2: Apply Stats to All Data Splits","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# Apply SAME stats to training data\ntrain_normalized = apply_normalization(train_labels, stats)\n\n# Apply SAME stats to validation data\nval_labels = [2.1, 4.3, 6.7, 9.2]\nval_normalized = apply_normalization(val_labels, stats)\n\n# Apply SAME stats to test data  \ntest_labels = [1.8, 5.1, 7.3]\ntest_normalized = apply_normalization(test_labels, stats)","category":"page"},{"location":"guide/#Step-3:-Denormalize-Predictions","page":"User Guide","title":"Step 3: Denormalize Predictions","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# After training your model on normalized data...\npredictions_normalized = model(X_test)  # Model outputs normalized predictions\n\n# Convert back to original scale using SAME stats\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"guide/#Multi-Target-Regression-(Stats-Based)","page":"User Guide","title":"Multi-Target Regression (Stats-Based)","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"For multiple target variables, follow the same three-step pattern:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# Training data: each column is a different target\ntrain_labels = [1.0 10.0 100.0;\n                5.0 20.0 200.0;\n                3.0 15.0 150.0;\n                8.0 25.0 250.0]\n\n# Step 1: Compute stats from training data ONLY\nstats = compute_normalization_stats(train_labels; mode=:columnwise, method=:zscore)\n\n# Step 2: Apply SAME stats to all splits\ntrain_normalized = apply_normalization(train_labels, stats)\nval_normalized = apply_normalization(val_labels, stats)    # Same stats\ntest_normalized = apply_normalization(test_labels, stats)  # Same stats\n\n# Step 3: Denormalize predictions using SAME stats\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"guide/#Normalization-Methods-(Stats-Based)","page":"User Guide","title":"Normalization Methods (Stats-Based)","text":"","category":"section"},{"location":"guide/#Min-Max-Normalization","page":"User Guide","title":"Min-Max Normalization","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Scales values to a specified range (default: [-1, 1]):","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# Step 1: Compute stats from training data\nstats = compute_normalization_stats(train_labels; method=:minmax, range=(-1, 1))\n\n# Step 2: Apply to all splits\ntrain_norm = apply_normalization(train_labels, stats)\ntest_norm = apply_normalization(test_labels, stats)\n\n# Step 3: Denormalize predictions\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"guide/#Z-Score-Normalization","page":"User Guide","title":"Z-Score Normalization","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Standardizes values to have zero mean and unit variance:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# Step 1: Compute stats from training data\nstats = compute_normalization_stats(train_labels; method=:zscore)\n\n# Step 2: Apply to all splits  \ntrain_norm = apply_normalization(train_labels, stats)\ntest_norm = apply_normalization(test_labels, stats)\n\n# Step 3: Denormalize predictions\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"guide/#Outlier-Handling-(Stats-Based)","page":"User Guide","title":"Outlier Handling (Stats-Based)","text":"","category":"section"},{"location":"guide/#Quantile-Based-Clipping","page":"User Guide","title":"Quantile-Based Clipping","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Configure clipping when computing stats from training data:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# Step 1: Compute stats with outlier clipping (training data only)\nstats = compute_normalization_stats(train_labels; \n    method=:zscore, \n    clip_quantiles=(0.01, 0.99)  # Default: clip to 1st-99th percentiles\n)\n\n# Step 2: Apply to all splits (same clipping applied)\ntrain_norm = apply_normalization(train_labels, stats)\ntest_norm = apply_normalization(test_labels, stats)\n\n# Step 3: Denormalize predictions\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"guide/#Why-Clip-Outliers?","page":"User Guide","title":"Why Clip Outliers?","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Outliers can severely distort normalization, especially min-max scaling:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"train_labels = [1, 2, 3, 4, 5, 1000]  # 1000 is an outlier\n\n# Step 1: Compute stats with clipping\nstats_with_clip = compute_normalization_stats(train_labels; clip_quantiles=(0.1, 0.9))\n\n# Step 2: Apply to test data\ntest_labels = [1.5, 2.5, 3.5]\ntest_norm = apply_normalization(test_labels, stats_with_clip)\n# Result: better distribution because outlier was clipped during stats computation","category":"page"},{"location":"guide/#Handling-Missing-Data-(Stats-Based)","page":"User Guide","title":"Handling Missing Data (Stats-Based)","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"NaN values are handled gracefully in the stats-based workflow:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# Training data with missing values\ntrain_with_nan = [1.0, 2.0, NaN, 4.0, 5.0, 100.0]\n\n# Step 1: Compute stats from valid training data only\nstats = compute_normalization_stats(train_with_nan)  # Uses [1.0, 2.0, 4.0, 5.0, 100.0]\n\n# Step 2: Apply to all splits (NaN positions preserved)\ntrain_norm = apply_normalization(train_with_nan, stats)  # NaNs preserved\ntest_norm = apply_normalization(test_with_nan, stats)    # NaNs preserved, same stats\n\n# Step 3: Denormalize predictions\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"guide/#Complete-Machine-Learning-Workflow","page":"User Guide","title":"Complete Machine Learning Workflow","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Here's the complete pattern for any ML project:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# Step 1: Compute normalization statistics on training data ONLY\ntrain_labels = [1.0, 2.0, 3.0, 4.0, 5.0, 100.0]  # With outlier\nstats = compute_normalization_stats(train_labels; method=:zscore, clip_quantiles=(0.01, 0.99))\n\n# Step 2: Apply SAME stats to all data splits\ntrain_normalized = apply_normalization(train_labels, stats)\nval_normalized = apply_normalization(val_labels, stats)    # Same stats\ntest_normalized = apply_normalization(test_labels, stats)  # Same stats\n\n# Step 3: Train model on normalized data\n# model = train_model(X_train, train_normalized)\n\n# Step 4: Make predictions and denormalize using SAME stats\npredictions_normalized = model(X_test)\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"guide/#Best-Practices-(Stats-Based-Workflow)","page":"User Guide","title":"Best Practices (Stats-Based Workflow)","text":"","category":"section"},{"location":"guide/#When-to-Use-Each-Method","page":"User Guide","title":"When to Use Each Method","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Min-max normalization: When you know the expected range of your data or want bounded outputs\nZ-score normalization: When your data is approximately normally distributed\nGlobal mode: When all targets should be on the same scale (e.g., related measurements)\nColumn-wise mode: When targets represent different quantities with different scales","category":"page"},{"location":"guide/#The-Golden-Rule:-Always-Use-Stats-Based-Workflow","page":"User Guide","title":"The Golden Rule: Always Use Stats-Based Workflow","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# ✅ CORRECT: Stats-based workflow (prevents data leakage)\nstats = compute_normalization_stats(train_labels)  # Training data only\ntrain_norm = apply_normalization(train_labels, stats)\ntest_norm = apply_normalization(test_labels, stats)  # Same stats\npredictions_original = denormalize_labels(predictions_normalized, stats)\n\n# ❌ WRONG: Direct normalization (causes data leakage)\n# train_norm = normalize_labels(train_labels)\n# test_norm = normalize_labels(test_labels)  # Different stats!","category":"page"},{"location":"guide/#Cross-Validation-with-Consistent-Stats","page":"User Guide","title":"Cross-Validation with Consistent Stats","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# For each CV fold, compute stats on training portion only\nfor fold in 1:5\n    train_idx, val_idx = get_cv_indices(fold)\n    \n    # Step 1: Compute stats on training fold only\n    fold_stats = compute_normalization_stats(y_train[train_idx])\n    \n    # Step 2: Apply to both training and validation portions\n    y_train_norm = apply_normalization(y_train[train_idx], fold_stats)\n    y_val_norm = apply_normalization(y_train[val_idx], fold_stats)  # Same stats!\n    \n    # Step 3: Train and validate model\n    model = train_model(X_train[train_idx], y_train_norm)\n    val_pred_norm = model(X_train[val_idx])\n    val_pred_original = denormalize_labels(val_pred_norm, fold_stats)\nend","category":"page"},{"location":"guide/#Handling-Extreme-Outliers","page":"User Guide","title":"Handling Extreme Outliers","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Configure clipping when computing stats from training data:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# For data with extreme outliers (e.g., financial data)\nstats = compute_normalization_stats(train_labels; clip_quantiles=(0.1, 0.9))\n\n# For very clean data, you might skip clipping\nstats = compute_normalization_stats(train_labels; clip_quantiles=nothing)\n\n# Apply same clipping to all splits\ntrain_norm = apply_normalization(train_labels, stats)\ntest_norm = apply_normalization(test_labels, stats)","category":"page"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Main-Functions","page":"API Reference","title":"Main Functions","text":"","category":"section"},{"location":"api/#RealLabelNormalization.normalize_labels","page":"API Reference","title":"RealLabelNormalization.normalize_labels","text":"normalize_labels(labels; method=:minmax, range=(-1, 1), mode=:global, clip_quantiles=(0.01, 0.99))\n\nNormalize labels with various normalization methods and modes. Handles NaN values by ignoring them  in statistical computations and preserving them in the output.\n\nArguments\n\nlabels: Vector or matrix where the last dimension is the number of samples\nmethod::Symbol: Normalization method\n:minmax: Min-max normalization (default)\n:zscore: Z-score normalization (mean=0, std=1)\nrange::Tuple{Real,Real}: Target range for min-max normalization (default: (-1, 1))\n(-1, 1): Scaled min-max to [-1,1] (default)\n(0, 1): Standard min-max to [0,1]\nCustom ranges: e.g., (-2, 2)\nmode::Symbol: Normalization scope\n:global: Normalize across all values (default)\n:columnwise: Normalize each column independently\n:rowwise: Normalize each row independently\nclip_quantiles::Union{Nothing,Tuple{Real,Real}}: Percentile values (0-1) for outlier clipping before normalization\n(0.01, 0.99): Clip to 1st-99th percentiles (default)\n(0.05, 0.95): Clip to 5th-95th percentiles (more aggressive)\nnothing: No clipping\n\nNaN Handling\n\nNaN values are ignored when computing statistics (min, max, mean, std, quantiles)\nNaN values are preserved in the output (remain as NaN)\nIf all values in a column are NaN, appropriate warnings are issued and NaN is returned\n\nReturns\n\nNormalized labels with same shape as input\n\nExamples\n\n# Vector labels (single target)\nlabels = [1.0, 5.0, 3.0, 8.0, 2.0, 100.0]  # 100.0 is outlier\n\n# Min-max to [-1,1] with outlier clipping (default)\nnormalized = normalize_labels(labels)\n\n# Min-max to [0,1] \nnormalized = normalize_labels(labels; range=(0, 1))\n\n# Z-score normalization with outlier clipping\nnormalized = normalize_labels(labels; method=:zscore)\n\n# Matrix labels (multi-target)\nlabels_matrix = [1.0 10.0; 5.0 20.0; 3.0 15.0; 8.0 25.0; 1000.0 5.0]  # Outlier in col 1\n\n# Global normalization with clipping\nnormalized = normalize_labels(labels_matrix; mode=:global)\n\n# Column-wise normalization with clipping \nnormalized = normalize_labels(labels_matrix; mode=:columnwise)\n\n# Row-wise normalization with clipping\nnormalized = normalize_labels(labels_matrix; mode=:rowwise)\n\n\n\n\n\n","category":"function"},{"location":"api/#RealLabelNormalization.compute_normalization_stats","page":"API Reference","title":"RealLabelNormalization.compute_normalization_stats","text":"compute_normalization_stats(labels; method=:minmax, mode=:global, \nrange=(-1, 1), clip_quantiles=(0.01, 0.99))\n\nCompute normalization statistics from training data for later application to validation/test sets.\n\nInputs\n\nlabels: Vector or matrix where the last dimension is the number of samples\nmethod::Symbol: Normalization method\n:minmax: Min-max normalization (default)\n:zscore: Z-score normalization (mean=0, std=1)\nrange::Tuple{Real,Real}: Target range for min-max normalization (default (-1, 1))\n(-1, 1): Scaled min-max to [-1,1] (default)\n(0, 1): Standard min-max to [0,1]\nCustom ranges: e.g., (-2, 2)\nmode::Symbol: Normalization scope\n:global: Normalize across all values (default)\n:columnwise: Normalize each column independently\n:rowwise: Normalize each row independently\nclip_quantiles::Union{Nothing,Tuple{Real,Real}}: Percentile values (0-1) for outlier clipping before normalization\n(0.01, 0.99): Clip to 1st-99th percentiles (default)\n(0.05, 0.95): Clip to 5th-95th percentiles (more aggressive)\nnothing: No clipping\n\nReturns\n\nNamed tuple with normalization parameters that can be used with apply_normalization\n\nExample\n\n# Compute stats from training data with outlier clipping\ntrain_stats = compute_normalization_stats(train_labels; method=:zscore, mode=:columnwise, clip_quantiles=(0.05, 0.95))\n\n# Apply to validation/test data (uses same clipping bounds)\nval_normalized = apply_normalization(val_labels, train_stats)\ntest_normalized = apply_normalization(test_labels, train_stats)\n\n\n\n\n\n","category":"function"},{"location":"api/#RealLabelNormalization.apply_normalization","page":"API Reference","title":"RealLabelNormalization.apply_normalization","text":"apply_normalization(labels, stats)\n\nApply pre-computed normalization statistics to new data (validation/test sets).\n\nEnsures consistent normalization across train/validation/test splits using only training statistics. This includes applying the same clipping bounds if they were used during training.\n\n\n\n\n\n","category":"function"},{"location":"api/#RealLabelNormalization.denormalize_labels","page":"API Reference","title":"RealLabelNormalization.denormalize_labels","text":"denormalize_labels(normalized_labels, stats)\n\nConvert normalized labels back to original scale using stored statistics.\n\nUseful for interpreting model predictions in original units.\n\n\n\n\n\n","category":"function"},{"location":"api/#Function-Index","page":"API Reference","title":"Function Index","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"","category":"page"},{"location":"api/#Internal-Implementation-Details","page":"API Reference","title":"Internal Implementation Details","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"The package is organized into several internal modules for different aspects of label normalization:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Clipping: Handles outlier detection and clipping based on quantiles\nMethods: Implements different normalization algorithms (min-max, z-score)\nStatistics: Computes and stores normalization statistics\nCore: Main API functions that orchestrate the normalization process","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"For details on the internal implementation, please refer to the source code in the package repository.","category":"page"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This page provides comprehensive examples of the stats-based workflow for leak-free label normalization.","category":"page"},{"location":"examples/#Critical:-Always-Use-the-Stats-Based-Workflow","page":"Examples","title":"⚠️ Critical: Always Use the Stats-Based Workflow","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"NEVER use normalize_labels() directly on your full dataset. This causes data leakage! Always follow the three-step pattern:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Compute stats from training data ONLY\nApply the same stats to validation/test data  \nDenormalize predictions using the same stats","category":"page"},{"location":"examples/#Basic-Examples","page":"Examples","title":"Basic Examples","text":"","category":"section"},{"location":"examples/#Example-1:-Single-Target-Regression-(Stats-Based)","page":"Examples","title":"Example 1: Single Target Regression (Stats-Based)","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using RealLabelNormalization\nusing Random\nRandom.seed!(42)\n\n# Simulate house prices with some outliers\ntrain_prices = [200_000, 250_000, 180_000, 320_000, 275_000, \n                190_000, 2_000_000, 210_000, 290_000, 240_000]  # 2M is outlier\ntest_prices = [220_000, 280_000, 195_000, 310_000]\n\nprintln(\"Training prices: \", train_prices)\nprintln(\"Test prices: \", test_prices)\n\n# Step 1: Compute stats from training data ONLY\nstats = compute_normalization_stats(train_prices; method=:zscore, clip_quantiles=(0.01, 0.99))\n\n# Step 2: Apply SAME stats to both training and test data\ntrain_normalized = apply_normalization(train_prices, stats)\ntest_normalized = apply_normalization(test_prices, stats)\n\nprintln(\"Training normalized: \", train_normalized)\nprintln(\"Test normalized: \", test_normalized)\n\n# Step 3: Denormalize predictions using SAME stats\npredictions_normalized = [0.5, -0.2, 0.8, 0.1]  # Model outputs\npredictions_original = denormalize_labels(predictions_normalized, stats)\nprintln(\"Predictions (original scale): \", predictions_original)","category":"page"},{"location":"examples/#Example-2:-Multi-Target-Regression-(Stats-Based)","page":"Examples","title":"Example 2: Multi-Target Regression (Stats-Based)","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Simulate multi-target regression: [temperature, humidity, pressure]\nweather_train = [20.5 65.0 1013.2;\n                22.1 58.3 1015.8;\n                18.9 72.1 1008.9;\n                25.4 45.2 1020.1;\n                19.2 68.7 1011.4;\n                50.0 30.0 950.0;   # Outlier row\n                21.8 61.5 1016.3]\n\nweather_test = [19.5 70.0 1012.5;\n               23.2 55.0 1018.0;\n               17.8 75.0 1009.5]\n\nprintln(\"Training data shape: \", size(weather_train))\nprintln(\"Test data shape: \", size(weather_test))\n\n# Step 1: Compute stats from training data ONLY\nstats_col = compute_normalization_stats(weather_train; mode=:columnwise, method=:zscore)\nstats_global = compute_normalization_stats(weather_train; mode=:global, method=:zscore)\n\n# Step 2: Apply SAME stats to both training and test data\ntrain_norm_col = apply_normalization(weather_train, stats_col)\ntest_norm_col = apply_normalization(weather_test, stats_col)\n\ntrain_norm_global = apply_normalization(weather_train, stats_global)\ntest_norm_global = apply_normalization(weather_test, stats_global)\n\nprintln(\"Column-wise normalized ranges (training):\")\nfor i in 1:size(train_norm_col, 2)\n    col_range = [minimum(train_norm_col[:, i]), maximum(train_norm_col[:, i])]\n    println(\"  Column $i: $col_range\")\nend\n\nprintln(\"Global normalized range (training): [$(minimum(train_norm_global)), $(maximum(train_norm_global))]\")\n\n# Step 3: Denormalize predictions using SAME stats\npredictions_norm = [0.5 -0.2 0.8; -0.3 0.7 -0.1]\npredictions_original = denormalize_labels(predictions_norm, stats_col)\nprintln(\"Predictions (original scale): \", predictions_original)","category":"page"},{"location":"examples/#Machine-Learning-Workflow-Examples","page":"Examples","title":"Machine Learning Workflow Examples","text":"","category":"section"},{"location":"examples/#Example-3:-Complete-Train/Validation/Test-Pipeline-(Stats-Based)","page":"Examples","title":"Example 3: Complete Train/Validation/Test Pipeline (Stats-Based)","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using RealLabelNormalization\nusing Random\nRandom.seed!(123)\n\n# Simulate a regression dataset\nn_samples = 1000\nn_features = 5\nX = randn(n_samples, n_features)\n\n# Target with some non-linear relationship and outliers\ny = 2 * X[:, 1] + 0.5 * X[:, 2].^2 - X[:, 3] + 0.1 * randn(n_samples)\n# Add a few outliers\ny[1:5] .+= 50 * randn(5)\n\n# Split data\ntrain_idx = 1:600\nval_idx = 601:800\ntest_idx = 801:1000\n\nX_train, y_train = X[train_idx, :], y[train_idx]\nX_val, y_val = X[val_idx, :], y[val_idx]\nX_test, y_test = X[test_idx, :], y[test_idx]\n\nprintln(\"Original target statistics:\")\nprintln(\"  Train: mean=$(mean(y_train)), std=$(std(y_train))\")\nprintln(\"  Val:   mean=$(mean(y_val)), std=$(std(y_val))\")\nprintln(\"  Test:  mean=$(mean(y_test)), std=$(std(y_test))\")\n\n# Step 1: Compute normalization statistics from training data ONLY\nstats = compute_normalization_stats(y_train; method=:zscore, clip_quantiles=(0.01, 0.99))\nprintln(\"\\\\nNormalization statistics computed from training data:\")\nprintln(stats)\n\n# Step 2: Apply SAME stats to all splits\ny_train_norm = apply_normalization(y_train, stats)\ny_val_norm = apply_normalization(y_val, stats)\ny_test_norm = apply_normalization(y_test, stats)\n\nprintln(\"\\\\nNormalized target statistics:\")\nprintln(\"  Train: mean=$(mean(y_train_norm)), std=$(std(y_train_norm))\")\nprintln(\"  Val:   mean=$(mean(y_val_norm)), std=$(std(y_val_norm))\")\nprintln(\"  Test:  mean=$(mean(y_test_norm)), std=$(std(y_test_norm))\")\n\n# Step 3: Train model on normalized data (placeholder)\n# model = fit_model(X_train, y_train_norm)\n# y_pred_norm = predict(model, X_test)\n\n# Simulate some predictions\ny_pred_norm = y_test_norm + 0.1 * randn(length(y_test_norm))  # Add some error\n\n# Step 4: Denormalize predictions back to original scale using SAME stats\ny_pred_original = denormalize_labels(y_pred_norm, stats)\n\nprintln(\"\\\\nPrediction comparison (first 10 samples):\")\nprintln(\"  True:      \", y_test[1:10])\nprintln(\"  Predicted: \", y_pred_original[1:10])\nprintln(\"  Error:     \", abs.(y_test[1:10] - y_pred_original[1:10]))","category":"page"},{"location":"examples/#Example-4:-Handling-Missing-Data-(Stats-Based)","page":"Examples","title":"Example 4: Handling Missing Data (Stats-Based)","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using RealLabelNormalization\n\n# Training data with missing values (NaN)\ntrain_with_missing = [1.0, 2.0, NaN, 4.0, 5.0, 6.0, NaN, 8.0, 100.0, 9.0]\ntest_with_missing = [1.5, NaN, 3.2, 4.8, NaN, 7.1]\n\nprintln(\"Training data: \", train_with_missing)\nprintln(\"Test data: \", test_with_missing)\nprintln(\"Valid training values: \", train_with_missing[.!isnan.(train_with_missing)])\n\n# Step 1: Compute stats from valid training data only\nstats = compute_normalization_stats(train_with_missing; method=:zscore, clip_quantiles=(0.01, 0.99))\nprintln(\"\\\\nComputed statistics from valid training data: \", stats)\n\n# Step 2: Apply SAME stats to both training and test data\ntrain_norm = apply_normalization(train_with_missing, stats)\ntest_norm = apply_normalization(test_with_missing, stats)\n\nprintln(\"Training normalized: \", train_norm)\nprintln(\"Test normalized: \", test_norm)\n\n# Check that NaN positions are preserved\nprintln(\"Training NaN preserved? \", isnan.(train_with_missing) == isnan.(train_norm))\nprintln(\"Test NaN preserved? \", isnan.(test_with_missing) == isnan.(test_norm))\n\n# Step 3: Denormalize predictions using SAME stats\npredictions_norm = [0.5, NaN, -0.2, 0.8, NaN, 0.1]\npredictions_original = denormalize_labels(predictions_norm, stats)\nprintln(\"Predictions (original scale): \", predictions_original)","category":"page"},{"location":"examples/#The-Golden-Rule:-Stats-Based-Workflow","page":"Examples","title":"The Golden Rule: Stats-Based Workflow","text":"","category":"section"},{"location":"examples/#CORRECT:-Always-Use-This-Pattern","page":"Examples","title":"✅ CORRECT: Always Use This Pattern","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Step 1: Compute stats from training data ONLY\nstats = compute_normalization_stats(train_labels; method=:zscore, clip_quantiles=(0.01, 0.99))\n\n# Step 2: Apply SAME stats to all data splits\ntrain_norm = apply_normalization(train_labels, stats)\nval_norm = apply_normalization(val_labels, stats)    # Same stats\ntest_norm = apply_normalization(test_labels, stats)  # Same stats\n\n# Step 3: Denormalize predictions using SAME stats\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"examples/#WRONG:-Direct-Normalization-(Causes-Data-Leakage)","page":"Examples","title":"❌ WRONG: Direct Normalization (Causes Data Leakage)","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"# DON'T DO THIS - causes data leakage!\ntrain_norm = normalize_labels(train_labels)\ntest_norm = normalize_labels(test_labels)  # Different stats = data leakage!","category":"page"},{"location":"examples/#Why-Stats-Based-Workflow-is-Critical","page":"Examples","title":"Why Stats-Based Workflow is Critical","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Prevents Data Leakage: Test data never influences normalization parameters\nConsistent Scaling: All data splits use identical normalization\nProper Validation: Model performance reflects real-world generalization\nCorrect Predictions: Denormalization uses the same parameters as training","category":"page"},{"location":"examples/#Advanced-Examples","page":"Examples","title":"Advanced Examples","text":"","category":"section"},{"location":"examples/#Example-5:-Cross-Validation-with-Consistent-Stats","page":"Examples","title":"Example 5: Cross-Validation with Consistent Stats","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using RealLabelNormalization\nusing Statistics\n\n# Simulate a dataset for cross-validation\nn_samples = 1000\nX = randn(n_samples, 5)\ny = 2 * X[:, 1] + 0.5 * X[:, 2].^2 - X[:, 3] + 0.1 * randn(n_samples)\n\n# 5-fold cross-validation\nn_folds = 5\nfold_size = n_samples ÷ n_folds\n\nfor fold in 1:n_folds\n    println(\"\\\\n=== Fold $fold ===\")\n    \n    # Split data for this fold\n    val_start = (fold - 1) * fold_size + 1\n    val_end = fold * fold_size\n    val_idx = val_start:val_end\n    train_idx = setdiff(1:n_samples, val_idx)\n    \n    X_train, y_train = X[train_idx, :], y[train_idx]\n    X_val, y_val = X[val_idx, :], y[val_idx]\n    \n    # Step 1: Compute stats from training fold ONLY\n    stats = compute_normalization_stats(y_train; method=:zscore, clip_quantiles=(0.01, 0.99))\n    \n    # Step 2: Apply SAME stats to both training and validation\n    y_train_norm = apply_normalization(y_train, stats)\n    y_val_norm = apply_normalization(y_val, stats)  # Same stats!\n    \n    println(\"Training stats: mean=$(mean(y_train)), std=$(std(y_train))\")\n    println(\"Validation stats: mean=$(mean(y_val)), std=$(std(y_val))\")\n    println(\"Normalized training: mean=$(mean(y_train_norm)), std=$(std(y_train_norm))\")\n    println(\"Normalized validation: mean=$(mean(y_val_norm)), std=$(std(y_val_norm))\")\n    \n    # Step 3: Train model and make predictions\n    # model = train_model(X_train, y_train_norm)\n    # val_pred_norm = model(X_val)\n    # val_pred_original = denormalize_labels(val_pred_norm, stats)\nend","category":"page"},{"location":"examples/#Example-6:-Custom-Normalization-Ranges","page":"Examples","title":"Example 6: Custom Normalization Ranges","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using RealLabelNormalization\n\n# Original data\ndata = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n\nprintln(\"Original data: \", data)\n\n# Different target ranges\nranges = [(-1, 1), (0, 1), (-2, 2), (-10, 10)]\n\nfor range in ranges\n    normalized = normalize_labels(data; range=range, clip_quantiles=nothing)\n    actual_range = (minimum(normalized), maximum(normalized))\n    println(\"Target $range -> Actual $actual_range\")\nend","category":"page"},{"location":"examples/#Example-7:-Multi-Target-with-Different-Scales","page":"Examples","title":"Example 7: Multi-Target with Different Scales","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using RealLabelNormalization\n\n# Multi-target data with very different scales\n# Column 1: Small values (0-10)\n# Column 2: Medium values (100-1000)  \n# Column 3: Large values (10000-100000)\nmulti_scale_data = [1.0 100.0 10000.0;\n                    2.0 200.0 20000.0;\n                    3.0 300.0 30000.0;\n                    4.0 400.0 40000.0;\n                    5.0 500.0 50000.0;\n                    100.0 50000.0 5000.0]  # Outlier row\n\nprintln(\"Original data ranges per column:\")\nfor i in 1:3\n    col_range = [minimum(multi_scale_data[:, i]), maximum(multi_scale_data[:, i])]\n    println(\"  Column $i: $col_range\")\nend\n\n# Compare global vs column-wise normalization\nglobal_norm = normalize_labels(multi_scale_data; mode=:global)\ncolumn_norm = normalize_labels(multi_scale_data; mode=:columnwise)\n\nprintln(\"\\\\nGlobal normalization - range per column:\")\nfor i in 1:3\n    col_range = [minimum(global_norm[:, i]), maximum(global_norm[:, i])]\n    println(\"  Column $i: $col_range\")\nend\n\nprintln(\"\\\\nColumn-wise normalization - range per column:\")\nfor i in 1:3\n    col_range = [minimum(column_norm[:, i]), maximum(column_norm[:, i])]\n    println(\"  Column $i: $col_range\")\nend","category":"page"},{"location":"examples/#Performance-Considerations","page":"Examples","title":"Performance Considerations","text":"","category":"section"},{"location":"examples/#Example-8:-Large-Dataset-Handling","page":"Examples","title":"Example 8: Large Dataset Handling","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using RealLabelNormalization\nusing BenchmarkTools\n\n# Simulate large dataset\nlarge_data = randn(100_000, 10)  # 100k samples, 10 targets\n\nprintln(\"Dataset size: \", size(large_data))\n\n# Benchmark different operations\nprintln(\"\\\\nPerformance benchmarks:\")\n@btime normalize_labels($large_data; mode=:columnwise)\n@btime compute_normalization_stats($large_data; mode=:columnwise)\n@btime apply_normalization($large_data, $stats) setup=(stats=compute_normalization_stats($large_data; mode=:columnwise))","category":"page"},{"location":"#RealLabelNormalization.jl","page":"Home","title":"RealLabelNormalization.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package for robust normalization of real-valued labels, commonly used in regression tasks. This package provides various normalization methods with built-in outlier handling and NaN support.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Multiple normalization methods: Min-max and Z-score normalization\nFlexible normalization modes: Global or column-wise normalization\nRobust outlier handling: Configurable quantile-based clipping\nNaN handling: Preserves NaN values while computing statistics on valid data\nConsistent train/test normalization: Save statistics from training data and apply to test data","category":"page"},{"location":"#Quick-Start-(Stats-Based-Workflow)","page":"Home","title":"Quick Start (Stats-Based Workflow)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using RealLabelNormalization\n\n# Training labels with outlier\ntrain_labels = [1.5, 2.3, 4.1, 3.7, 5.2, 100.0]\ntest_labels = [2.1, 3.9, 4.5]\n\n# Step 1: Compute stats from TRAINING DATA ONLY\nstats = compute_normalization_stats(train_labels; method=:zscore, clip_quantiles=(0.01, 0.99))\n\n# Step 2: Apply SAME stats to training data\ntrain_normalized = apply_normalization(train_labels, stats)\n\n# Step 3: Apply SAME STATS to test data (prevents data leakage!)\ntest_normalized = apply_normalization(test_labels, stats)\n\n# Step 4: Train model on normalized data\n# model = train_your_model(X_train, train_normalized)\n\n# Step 5: Denormalize predictions back to original scale using SAME stats\npredictions_normalized = model(X_test)  # Model outputs normalized predictions\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"RealLabelNormalization\")","category":"page"},{"location":"#API-Reference","page":"Home","title":"API Reference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#RealLabelNormalization._apply_training_clip_bounds-Tuple{AbstractArray, NamedTuple}","page":"Home","title":"RealLabelNormalization._apply_training_clip_bounds","text":"Apply training clip bounds to validation/test data.\n\n\n\n\n\n","category":"method"},{"location":"#RealLabelNormalization._clip_outliers-Tuple{AbstractVector, Tuple{Real, Real}, Symbol}","page":"Home","title":"RealLabelNormalization._clip_outliers","text":"Clip outliers using quantiles before normalization.\n\n\n\n\n\n","category":"method"},{"location":"#RealLabelNormalization.apply_normalization-Tuple{AbstractArray, NamedTuple}","page":"Home","title":"RealLabelNormalization.apply_normalization","text":"apply_normalization(labels, stats)\n\nApply pre-computed normalization statistics to new data (validation/test sets).\n\nEnsures consistent normalization across train/validation/test splits using only training statistics. This includes applying the same clipping bounds if they were used during training.\n\n\n\n\n\n","category":"method"},{"location":"#RealLabelNormalization.compute_normalization_stats-Tuple{AbstractArray}","page":"Home","title":"RealLabelNormalization.compute_normalization_stats","text":"compute_normalization_stats(labels; method=:minmax, mode=:global, \nrange=(-1, 1), clip_quantiles=(0.01, 0.99))\n\nCompute normalization statistics from training data for later application to validation/test sets.\n\nInputs\n\nlabels: Vector or matrix where the last dimension is the number of samples\nmethod::Symbol: Normalization method\n:minmax: Min-max normalization (default)\n:zscore: Z-score normalization (mean=0, std=1)\nrange::Tuple{Real,Real}: Target range for min-max normalization (default (-1, 1))\n(-1, 1): Scaled min-max to [-1,1] (default)\n(0, 1): Standard min-max to [0,1]\nCustom ranges: e.g., (-2, 2)\nmode::Symbol: Normalization scope\n:global: Normalize across all values (default)\n:columnwise: Normalize each column independently\n:rowwise: Normalize each row independently\nclip_quantiles::Union{Nothing,Tuple{Real,Real}}: Percentile values (0-1) for outlier clipping before normalization\n(0.01, 0.99): Clip to 1st-99th percentiles (default)\n(0.05, 0.95): Clip to 5th-95th percentiles (more aggressive)\nnothing: No clipping\n\nReturns\n\nNamed tuple with normalization parameters that can be used with apply_normalization\n\nExample\n\n# Compute stats from training data with outlier clipping\ntrain_stats = compute_normalization_stats(train_labels; method=:zscore, mode=:columnwise, clip_quantiles=(0.05, 0.95))\n\n# Apply to validation/test data (uses same clipping bounds)\nval_normalized = apply_normalization(val_labels, train_stats)\ntest_normalized = apply_normalization(test_labels, train_stats)\n\n\n\n\n\n","category":"method"},{"location":"#RealLabelNormalization.denormalize_labels-Tuple{AbstractArray, NamedTuple}","page":"Home","title":"RealLabelNormalization.denormalize_labels","text":"denormalize_labels(normalized_labels, stats)\n\nConvert normalized labels back to original scale using stored statistics.\n\nUseful for interpreting model predictions in original units.\n\n\n\n\n\n","category":"method"},{"location":"#RealLabelNormalization.normalize_labels-Tuple{AbstractArray}","page":"Home","title":"RealLabelNormalization.normalize_labels","text":"normalize_labels(labels; method=:minmax, range=(-1, 1), mode=:global, clip_quantiles=(0.01, 0.99))\n\nNormalize labels with various normalization methods and modes. Handles NaN values by ignoring them  in statistical computations and preserving them in the output.\n\nArguments\n\nlabels: Vector or matrix where the last dimension is the number of samples\nmethod::Symbol: Normalization method\n:minmax: Min-max normalization (default)\n:zscore: Z-score normalization (mean=0, std=1)\nrange::Tuple{Real,Real}: Target range for min-max normalization (default: (-1, 1))\n(-1, 1): Scaled min-max to [-1,1] (default)\n(0, 1): Standard min-max to [0,1]\nCustom ranges: e.g., (-2, 2)\nmode::Symbol: Normalization scope\n:global: Normalize across all values (default)\n:columnwise: Normalize each column independently\n:rowwise: Normalize each row independently\nclip_quantiles::Union{Nothing,Tuple{Real,Real}}: Percentile values (0-1) for outlier clipping before normalization\n(0.01, 0.99): Clip to 1st-99th percentiles (default)\n(0.05, 0.95): Clip to 5th-95th percentiles (more aggressive)\nnothing: No clipping\n\nNaN Handling\n\nNaN values are ignored when computing statistics (min, max, mean, std, quantiles)\nNaN values are preserved in the output (remain as NaN)\nIf all values in a column are NaN, appropriate warnings are issued and NaN is returned\n\nReturns\n\nNormalized labels with same shape as input\n\nExamples\n\n# Vector labels (single target)\nlabels = [1.0, 5.0, 3.0, 8.0, 2.0, 100.0]  # 100.0 is outlier\n\n# Min-max to [-1,1] with outlier clipping (default)\nnormalized = normalize_labels(labels)\n\n# Min-max to [0,1] \nnormalized = normalize_labels(labels; range=(0, 1))\n\n# Z-score normalization with outlier clipping\nnormalized = normalize_labels(labels; method=:zscore)\n\n# Matrix labels (multi-target)\nlabels_matrix = [1.0 10.0; 5.0 20.0; 3.0 15.0; 8.0 25.0; 1000.0 5.0]  # Outlier in col 1\n\n# Global normalization with clipping\nnormalized = normalize_labels(labels_matrix; mode=:global)\n\n# Column-wise normalization with clipping \nnormalized = normalize_labels(labels_matrix; mode=:columnwise)\n\n# Row-wise normalization with clipping\nnormalized = normalize_labels(labels_matrix; mode=:rowwise)\n\n\n\n\n\n","category":"method"}]
}
