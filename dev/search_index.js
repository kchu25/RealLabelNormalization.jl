var documenterSearchIndex = {"docs":
[{"location":"guide/#User-Guide","page":"User Guide","title":"User Guide","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"This guide covers the stats-based workflow for leak-free label normalization in machine learning.","category":"page"},{"location":"guide/#Critical:-Always-Use-the-Stats-Based-Workflow","page":"User Guide","title":"⚠️ Critical: Always Use the Stats-Based Workflow","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"NEVER use normalize_labels() directly on your full dataset. This causes data leakage! Instead, follow this pattern:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Compute stats from training data ONLY\nApply the same stats to validation/test data\nDenormalize predictions using the same stats","category":"page"},{"location":"guide/#The-Three-Step-Pattern","page":"User Guide","title":"The Three-Step Pattern","text":"","category":"section"},{"location":"guide/#Step-1:-Compute-Normalization-Statistics-(Training-Data-Only)","page":"User Guide","title":"Step 1: Compute Normalization Statistics (Training Data Only)","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"using RealLabelNormalization\n\n# Your training labels (with outliers)\ntrain_labels = [1.2, 5.8, 3.4, 8.1, 2.3, 100.5]  # 100.5 is an outlier\n\n# Compute stats from training data ONLY\nstats = compute_normalization_stats(train_labels; method=:zscore, clip_quantiles=(0.01, 0.99))","category":"page"},{"location":"guide/#Step-2:-Apply-Stats-to-All-Data-Splits","page":"User Guide","title":"Step 2: Apply Stats to All Data Splits","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# Apply SAME stats to training data\ntrain_normalized = apply_normalization(train_labels, stats)\n\n# Apply SAME stats to validation data\nval_labels = [2.1, 4.3, 6.7, 9.2]\nval_normalized = apply_normalization(val_labels, stats)\n\n# Apply SAME stats to test data  \ntest_labels = [1.8, 5.1, 7.3]\ntest_normalized = apply_normalization(test_labels, stats)","category":"page"},{"location":"guide/#Step-3:-Denormalize-Predictions","page":"User Guide","title":"Step 3: Denormalize Predictions","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# After training your model on normalized data...\npredictions_normalized = model(X_test)  # Model outputs normalized predictions\n\n# Convert back to original scale using SAME stats\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"guide/#Multi-Target-Regression-(Stats-Based)","page":"User Guide","title":"Multi-Target Regression (Stats-Based)","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"For multiple target variables, follow the same three-step pattern:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# Training data: each column is a different target\ntrain_labels = [1.0 10.0 100.0;\n                5.0 20.0 200.0;\n                3.0 15.0 150.0;\n                8.0 25.0 250.0]\n\n# Step 1: Compute stats from training data ONLY\nstats = compute_normalization_stats(train_labels; mode=:columnwise, method=:zscore)\n\n# Step 2: Apply SAME stats to all splits\ntrain_normalized = apply_normalization(train_labels, stats)\nval_normalized = apply_normalization(val_labels, stats)    # Same stats\ntest_normalized = apply_normalization(test_labels, stats)  # Same stats\n\n# Step 3: Denormalize predictions using SAME stats\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"guide/#Normalization-Methods-(Stats-Based)","page":"User Guide","title":"Normalization Methods (Stats-Based)","text":"","category":"section"},{"location":"guide/#Min-Max-Normalization","page":"User Guide","title":"Min-Max Normalization","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Scales values to a specified range (default: [-1, 1]):","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# Step 1: Compute stats from training data\nstats = compute_normalization_stats(train_labels; method=:minmax, range=(-1, 1))\n\n# Step 2: Apply to all splits\ntrain_norm = apply_normalization(train_labels, stats)\ntest_norm = apply_normalization(test_labels, stats)\n\n# Step 3: Denormalize predictions\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"guide/#Z-Score-Normalization","page":"User Guide","title":"Z-Score Normalization","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Standardizes values to have zero mean and unit variance:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# Step 1: Compute stats from training data\nstats = compute_normalization_stats(train_labels; method=:zscore)\n\n# Step 2: Apply to all splits  \ntrain_norm = apply_normalization(train_labels, stats)\ntest_norm = apply_normalization(test_labels, stats)\n\n# Step 3: Denormalize predictions\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"guide/#Outlier-Handling-(Stats-Based)","page":"User Guide","title":"Outlier Handling (Stats-Based)","text":"","category":"section"},{"location":"guide/#Quantile-Based-Clipping","page":"User Guide","title":"Quantile-Based Clipping","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Configure clipping when computing stats from training data:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# Step 1: Compute stats with outlier clipping (training data only)\nstats = compute_normalization_stats(train_labels; \n    method=:zscore, \n    clip_quantiles=(0.01, 0.99)  # Default: clip to 1st-99th percentiles\n)\n\n# Step 2: Apply to all splits (same clipping applied)\ntrain_norm = apply_normalization(train_labels, stats)\ntest_norm = apply_normalization(test_labels, stats)\n\n# Step 3: Denormalize predictions\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"guide/#Why-Clip-Outliers?","page":"User Guide","title":"Why Clip Outliers?","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Outliers can severely distort normalization, especially min-max scaling:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"train_labels = [1, 2, 3, 4, 5, 1000]  # 1000 is an outlier\n\n# Step 1: Compute stats with clipping\nstats_with_clip = compute_normalization_stats(train_labels; clip_quantiles=(0.1, 0.9))\n\n# Step 2: Apply to test data\ntest_labels = [1.5, 2.5, 3.5]\ntest_norm = apply_normalization(test_labels, stats_with_clip)\n# Result: better distribution because outlier was clipped during stats computation","category":"page"},{"location":"guide/#Handling-Missing-Data-(Stats-Based)","page":"User Guide","title":"Handling Missing Data (Stats-Based)","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"NaN values are handled gracefully in the stats-based workflow:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# Training data with missing values\ntrain_with_nan = [1.0, 2.0, NaN, 4.0, 5.0, 100.0]\n\n# Step 1: Compute stats from valid training data only\nstats = compute_normalization_stats(train_with_nan)  # Uses [1.0, 2.0, 4.0, 5.0, 100.0]\n\n# Step 2: Apply to all splits (NaN positions preserved)\ntrain_norm = apply_normalization(train_with_nan, stats)  # NaNs preserved\ntest_norm = apply_normalization(test_with_nan, stats)    # NaNs preserved, same stats\n\n# Step 3: Denormalize predictions\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"guide/#Complete-Machine-Learning-Workflow","page":"User Guide","title":"Complete Machine Learning Workflow","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Here's the complete pattern for any ML project:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# Step 1: Compute normalization statistics on training data ONLY\ntrain_labels = [1.0, 2.0, 3.0, 4.0, 5.0, 100.0]  # With outlier\nstats = compute_normalization_stats(train_labels; method=:zscore, clip_quantiles=(0.01, 0.99))\n\n# Step 2: Apply SAME stats to all data splits\ntrain_normalized = apply_normalization(train_labels, stats)\nval_normalized = apply_normalization(val_labels, stats)    # Same stats\ntest_normalized = apply_normalization(test_labels, stats)  # Same stats\n\n# Step 3: Train model on normalized data\n# model = train_model(X_train, train_normalized)\n\n# Step 4: Make predictions and denormalize using SAME stats\npredictions_normalized = model(X_test)\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"guide/#Best-Practices-(Stats-Based-Workflow)","page":"User Guide","title":"Best Practices (Stats-Based Workflow)","text":"","category":"section"},{"location":"guide/#When-to-Use-Each-Method","page":"User Guide","title":"When to Use Each Method","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Min-max normalization: When you know the expected range of your data or want bounded outputs\nZ-score normalization: When your data is approximately normally distributed\nGlobal mode: When all targets should be on the same scale (e.g., related measurements)\nColumn-wise mode: When targets represent different quantities with different scales","category":"page"},{"location":"guide/#The-Golden-Rule:-Always-Use-Stats-Based-Workflow","page":"User Guide","title":"The Golden Rule: Always Use Stats-Based Workflow","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# ✅ CORRECT: Stats-based workflow (prevents data leakage)\nstats = compute_normalization_stats(train_labels)  # Training data only\ntrain_norm = apply_normalization(train_labels, stats)\ntest_norm = apply_normalization(test_labels, stats)  # Same stats\npredictions_original = denormalize_labels(predictions_normalized, stats)\n\n# ❌ WRONG: Direct normalization (causes data leakage)\n# train_norm = normalize_labels(train_labels)\n# test_norm = normalize_labels(test_labels)  # Different stats!","category":"page"},{"location":"guide/#Cross-Validation-with-Consistent-Stats","page":"User Guide","title":"Cross-Validation with Consistent Stats","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# For each CV fold, compute stats on training portion only\nfor fold in 1:5\n    train_idx, val_idx = get_cv_indices(fold)\n    \n    # Step 1: Compute stats on training fold only\n    fold_stats = compute_normalization_stats(y_train[train_idx])\n    \n    # Step 2: Apply to both training and validation portions\n    y_train_norm = apply_normalization(y_train[train_idx], fold_stats)\n    y_val_norm = apply_normalization(y_train[val_idx], fold_stats)  # Same stats!\n    \n    # Step 3: Train and validate model\n    model = train_model(X_train[train_idx], y_train_norm)\n    val_pred_norm = model(X_train[val_idx])\n    val_pred_original = denormalize_labels(val_pred_norm, fold_stats)\nend","category":"page"},{"location":"guide/#Handling-Extreme-Outliers","page":"User Guide","title":"Handling Extreme Outliers","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Configure clipping when computing stats from training data:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"# For data with extreme outliers (e.g., financial data)\nstats = compute_normalization_stats(train_labels; clip_quantiles=(0.1, 0.9))\n\n# For very clean data, you might skip clipping\nstats = compute_normalization_stats(train_labels; clip_quantiles=nothing)\n\n# Apply same clipping to all splits\ntrain_norm = apply_normalization(train_labels, stats)\ntest_norm = apply_normalization(test_labels, stats)","category":"page"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Main-Functions","page":"API Reference","title":"Main Functions","text":"","category":"section"},{"location":"api/#RealLabelNormalization.normalize_labels","page":"API Reference","title":"RealLabelNormalization.normalize_labels","text":"normalize_labels(labels; method=:minmax, range=(-1, 1), mode=:global, clip_quantiles=(0.01, 0.99), log_shift=100.0)\n\nNormalize labels with various normalization methods and modes. Handles NaN values by ignoring them  in statistical computations and preserving them in the output.\n\nArguments\n\nlabels: Vector or matrix where the last dimension is the number of samples\nmethod::Symbol: Normalization method\n:minmax: Min-max normalization (default)\n:zscore: Z-score normalization (mean=0, std=1)\n:log: Log normalization (log-transform with automatic offset for non-positive values)\nrange::Tuple{Real,Real}: Target range for min-max normalization (default: (-1, 1))\n(-1, 1): Scaled min-max to [-1,1] (default)\n(0, 1): Standard min-max to [0,1]\nCustom ranges: e.g., (-2, 2)\nNote: Ignored for :zscore and :log methods\nmode::Symbol: Normalization scope\n:global: Normalize across all values (default)\n:columnwise: Normalize each column independently\n:rowwise: Normalize each row independently\nclip_quantiles::Union{Nothing,Tuple{Real,Real}}: Percentile values (0-1) for outlier clipping before normalization\n(0.01, 0.99): Clip to 1st-99th percentiles (default)\n(0.05, 0.95): Clip to 5th-95th percentiles (more aggressive)\nnothing: No clipping\nlog_shift::Real: Shift parameter for log normalization (default: 100.0)\nFor :log method, the offset is computed as: offset = min_val <= 0 ? abs(min_val) + log_shift : 0.0\nLarger values make log normalization less sensitive to small values near zero\nOnly used when method=:log, ignored otherwise\n\nNaN Handling\n\nNaN values are ignored when computing statistics (min, max, mean, std, quantiles)\nNaN values are preserved in the output (remain as NaN)\nIf all values in a column are NaN, appropriate warnings are issued and NaN is returned\n\nReturns\n\nNormalized labels with same shape as input\n\nExamples\n\n# Vector labels (single target)\nlabels = [1.0, 5.0, 3.0, 8.0, 2.0, 100.0]  # 100.0 is outlier\n\n# Min-max to [-1,1] with outlier clipping (default)\nnormalized = normalize_labels(labels)\n\n# Min-max to [0,1] \nnormalized = normalize_labels(labels; range=(0, 1))\n\n# Z-score normalization with outlier clipping\nnormalized = normalize_labels(labels; method=:zscore)\n\n# Log normalization (useful for skewed distributions)\nnormalized = normalize_labels(labels; method=:log)\n\n# Log normalization with custom shift (less sensitive to small values)\nnormalized = normalize_labels(labels; method=:log, log_shift=1000.0)\n\n# Matrix labels (multi-target)\nlabels_matrix = [1.0 10.0; 5.0 20.0; 3.0 15.0; 8.0 25.0; 1000.0 5.0]  # Outlier in col 1\n\n# Global normalization with clipping\nnormalized = normalize_labels(labels_matrix; mode=:global)\n\n# Column-wise normalization with clipping \nnormalized = normalize_labels(labels_matrix; mode=:columnwise)\n\n# Row-wise normalization with clipping\nnormalized = normalize_labels(labels_matrix; mode=:rowwise)\n\n\n\n\n\n","category":"function"},{"location":"api/#RealLabelNormalization.compute_normalization_stats","page":"API Reference","title":"RealLabelNormalization.compute_normalization_stats","text":"compute_normalization_stats(labels; method=:minmax, mode=:global, \nrange=(-1, 1), clip_quantiles=(0.01, 0.99), log_shift=100.0)\n\nCompute normalization statistics from training data for later application to validation/test sets.\n\nInputs\n\nlabels: Vector or matrix where the last dimension is the number of samples\nmethod::Symbol: Normalization method\n:minmax: Min-max normalization (default)\n:zscore: Z-score normalization (mean=0, std=1)\n:log: Log normalization (log-transform with automatic offset for non-positive values)\nrange::Tuple{Real,Real}: Target range for min-max normalization (default (-1, 1))\n(-1, 1): Scaled min-max to [-1,1] (default)\n(0, 1): Standard min-max to [0,1]\nCustom ranges: e.g., (-2, 2)\nNote: Ignored for :zscore and :log methods\nmode::Symbol: Normalization scope\n:global: Normalize across all values (default)\n:columnwise: Normalize each column independently\n:rowwise: Normalize each row independently\nclip_quantiles::Union{Nothing,Tuple{Real,Real}}: Percentile values (0-1) for outlier clipping before normalization\n(0.01, 0.99): Clip to 1st-99th percentiles (default)\n(0.05, 0.95): Clip to 5th-95th percentiles (more aggressive)\nnothing: No clipping\nlog_shift::Real: Shift parameter for log normalization (default: 100.0)\nFor :log method, the offset is computed as: offset = min_val <= 0 ? abs(min_val) + log_shift : 0.0\nLarger values make log normalization less sensitive to small values near zero\nOnly used when method=:log, ignored otherwise\n\nReturns\n\nNamed tuple with normalization parameters that can be used with apply_normalization\n\nExample\n\n# Compute stats from training data with outlier clipping\ntrain_stats = compute_normalization_stats(train_labels; method=:zscore, mode=:columnwise, clip_quantiles=(0.05, 0.95))\n\n# Apply to validation/test data (uses same clipping bounds)\nval_normalized = apply_normalization(val_labels, train_stats)\ntest_normalized = apply_normalization(test_labels, train_stats)\n\n# Log normalization for skewed distributions with custom shift\nlog_stats = compute_normalization_stats(train_labels; method=:log, log_shift=1000.0)\nval_log_normalized = apply_normalization(val_labels, log_stats)\n\n\n\n\n\n","category":"function"},{"location":"api/#RealLabelNormalization.apply_normalization","page":"API Reference","title":"RealLabelNormalization.apply_normalization","text":"apply_normalization(labels, stats)\n\nApply pre-computed normalization statistics to new data (validation/test sets).\n\nEnsures consistent normalization across train/validation/test splits using only training statistics. This includes applying the same clipping bounds if they were used during training.\n\n\n\n\n\n","category":"function"},{"location":"api/#RealLabelNormalization.denormalize_labels","page":"API Reference","title":"RealLabelNormalization.denormalize_labels","text":"denormalize_labels(normalized_labels, stats)\n\nConvert normalized labels back to original scale using stored statistics.\n\nUseful for interpreting model predictions in original units.\n\nUses the GPU-compatible functor stored in stats.scalebackfunctor.\n\n\n\n\n\n","category":"function"},{"location":"api/#Function-Index","page":"API Reference","title":"Function Index","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"","category":"page"},{"location":"api/#Internal-Implementation-Details","page":"API Reference","title":"Internal Implementation Details","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"The package is organized into several internal modules for different aspects of label normalization:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Clipping: Handles outlier detection and clipping based on quantiles\nMethods: Implements different normalization algorithms (min-max, z-score)\nStatistics: Computes and stores normalization statistics\nCore: Main API functions that orchestrate the normalization process","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"For details on the internal implementation, please refer to the source code in the package repository.","category":"page"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This page provides comprehensive examples of the stats-based workflow for leak-free label normalization.","category":"page"},{"location":"examples/#Critical:-Always-Use-the-Stats-Based-Workflow","page":"Examples","title":"⚠️ Critical: Always Use the Stats-Based Workflow","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"NEVER use normalize_labels() directly on your full dataset. This causes data leakage! Always follow the three-step pattern:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Compute stats from training data ONLY\nApply the same stats to validation/test data  \nDenormalize predictions using the same stats","category":"page"},{"location":"examples/#Basic-Examples","page":"Examples","title":"Basic Examples","text":"","category":"section"},{"location":"examples/#Example-1:-Single-Target-Regression-(Stats-Based)","page":"Examples","title":"Example 1: Single Target Regression (Stats-Based)","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using RealLabelNormalization\nusing Random\nRandom.seed!(42)\n\n# Simulate house prices with some outliers\ntrain_prices = [200_000, 250_000, 180_000, 320_000, 275_000, \n                190_000, 2_000_000, 210_000, 290_000, 240_000]  # 2M is outlier\ntest_prices = [220_000, 280_000, 195_000, 310_000]\n\nprintln(\"Training prices: \", train_prices)\nprintln(\"Test prices: \", test_prices)\n\n# Step 1: Compute stats from training data ONLY\nstats = compute_normalization_stats(train_prices; method=:zscore, clip_quantiles=(0.01, 0.99))\n\n# Step 2: Apply SAME stats to both training and test data\ntrain_normalized = apply_normalization(train_prices, stats)\ntest_normalized = apply_normalization(test_prices, stats)\n\nprintln(\"Training normalized: \", train_normalized)\nprintln(\"Test normalized: \", test_normalized)\n\n# Step 3: Denormalize predictions using SAME stats\npredictions_normalized = [0.5, -0.2, 0.8, 0.1]  # Model outputs\npredictions_original = denormalize_labels(predictions_normalized, stats)\nprintln(\"Predictions (original scale): \", predictions_original)","category":"page"},{"location":"examples/#Example-2:-Multi-Target-Regression-(Stats-Based)","page":"Examples","title":"Example 2: Multi-Target Regression (Stats-Based)","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Simulate multi-target regression: [temperature, humidity, pressure]\nweather_train = [20.5 65.0 1013.2;\n                22.1 58.3 1015.8;\n                18.9 72.1 1008.9;\n                25.4 45.2 1020.1;\n                19.2 68.7 1011.4;\n                50.0 30.0 950.0;   # Outlier row\n                21.8 61.5 1016.3]\n\nweather_test = [19.5 70.0 1012.5;\n               23.2 55.0 1018.0;\n               17.8 75.0 1009.5]\n\nprintln(\"Training data shape: \", size(weather_train))\nprintln(\"Test data shape: \", size(weather_test))\n\n# Step 1: Compute stats from training data ONLY\nstats_col = compute_normalization_stats(weather_train; mode=:columnwise, method=:zscore)\nstats_global = compute_normalization_stats(weather_train; mode=:global, method=:zscore)\n\n# Step 2: Apply SAME stats to both training and test data\ntrain_norm_col = apply_normalization(weather_train, stats_col)\ntest_norm_col = apply_normalization(weather_test, stats_col)\n\ntrain_norm_global = apply_normalization(weather_train, stats_global)\ntest_norm_global = apply_normalization(weather_test, stats_global)\n\nprintln(\"Column-wise normalized ranges (training):\")\nfor i in 1:size(train_norm_col, 2)\n    col_range = [minimum(train_norm_col[:, i]), maximum(train_norm_col[:, i])]\n    println(\"  Column $i: $col_range\")\nend\n\nprintln(\"Global normalized range (training): [$(minimum(train_norm_global)), $(maximum(train_norm_global))]\")\n\n# Step 3: Denormalize predictions using SAME stats\npredictions_norm = [0.5 -0.2 0.8; -0.3 0.7 -0.1]\npredictions_original = denormalize_labels(predictions_norm, stats_col)\nprintln(\"Predictions (original scale): \", predictions_original)","category":"page"},{"location":"examples/#Machine-Learning-Workflow-Examples","page":"Examples","title":"Machine Learning Workflow Examples","text":"","category":"section"},{"location":"examples/#Example-3:-Complete-Train/Validation/Test-Pipeline-(Stats-Based)","page":"Examples","title":"Example 3: Complete Train/Validation/Test Pipeline (Stats-Based)","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using RealLabelNormalization\nusing Random\nRandom.seed!(123)\n\n# Simulate a regression dataset\nn_samples = 1000\nn_features = 5\nX = randn(n_samples, n_features)\n\n# Target with some non-linear relationship and outliers\ny = 2 * X[:, 1] + 0.5 * X[:, 2].^2 - X[:, 3] + 0.1 * randn(n_samples)\n# Add a few outliers\ny[1:5] .+= 50 * randn(5)\n\n# Split data\ntrain_idx = 1:600\nval_idx = 601:800\ntest_idx = 801:1000\n\nX_train, y_train = X[train_idx, :], y[train_idx]\nX_val, y_val = X[val_idx, :], y[val_idx]\nX_test, y_test = X[test_idx, :], y[test_idx]\n\nprintln(\"Original target statistics:\")\nprintln(\"  Train: mean=$(mean(y_train)), std=$(std(y_train))\")\nprintln(\"  Val:   mean=$(mean(y_val)), std=$(std(y_val))\")\nprintln(\"  Test:  mean=$(mean(y_test)), std=$(std(y_test))\")\n\n# Step 1: Compute normalization statistics from training data ONLY\nstats = compute_normalization_stats(y_train; method=:zscore, clip_quantiles=(0.01, 0.99))\nprintln(\"\\\\nNormalization statistics computed from training data:\")\nprintln(stats)\n\n# Step 2: Apply SAME stats to all splits\ny_train_norm = apply_normalization(y_train, stats)\ny_val_norm = apply_normalization(y_val, stats)\ny_test_norm = apply_normalization(y_test, stats)\n\nprintln(\"\\\\nNormalized target statistics:\")\nprintln(\"  Train: mean=$(mean(y_train_norm)), std=$(std(y_train_norm))\")\nprintln(\"  Val:   mean=$(mean(y_val_norm)), std=$(std(y_val_norm))\")\nprintln(\"  Test:  mean=$(mean(y_test_norm)), std=$(std(y_test_norm))\")\n\n# Step 3: Train model on normalized data (placeholder)\n# model = fit_model(X_train, y_train_norm)\n# y_pred_norm = predict(model, X_test)\n\n# Simulate some predictions\ny_pred_norm = y_test_norm + 0.1 * randn(length(y_test_norm))  # Add some error\n\n# Step 4: Denormalize predictions back to original scale using SAME stats\ny_pred_original = denormalize_labels(y_pred_norm, stats)\n\nprintln(\"\\\\nPrediction comparison (first 10 samples):\")\nprintln(\"  True:      \", y_test[1:10])\nprintln(\"  Predicted: \", y_pred_original[1:10])\nprintln(\"  Error:     \", abs.(y_test[1:10] - y_pred_original[1:10]))","category":"page"},{"location":"examples/#Example-4:-Handling-Missing-Data-(Stats-Based)","page":"Examples","title":"Example 4: Handling Missing Data (Stats-Based)","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using RealLabelNormalization\n\n# Training data with missing values (NaN)\ntrain_with_missing = [1.0, 2.0, NaN, 4.0, 5.0, 6.0, NaN, 8.0, 100.0, 9.0]\ntest_with_missing = [1.5, NaN, 3.2, 4.8, NaN, 7.1]\n\nprintln(\"Training data: \", train_with_missing)\nprintln(\"Test data: \", test_with_missing)\nprintln(\"Valid training values: \", train_with_missing[.!isnan.(train_with_missing)])\n\n# Step 1: Compute stats from valid training data only\nstats = compute_normalization_stats(train_with_missing; method=:zscore, clip_quantiles=(0.01, 0.99))\nprintln(\"\\\\nComputed statistics from valid training data: \", stats)\n\n# Step 2: Apply SAME stats to both training and test data\ntrain_norm = apply_normalization(train_with_missing, stats)\ntest_norm = apply_normalization(test_with_missing, stats)\n\nprintln(\"Training normalized: \", train_norm)\nprintln(\"Test normalized: \", test_norm)\n\n# Check that NaN positions are preserved\nprintln(\"Training NaN preserved? \", isnan.(train_with_missing) == isnan.(train_norm))\nprintln(\"Test NaN preserved? \", isnan.(test_with_missing) == isnan.(test_norm))\n\n# Step 3: Denormalize predictions using SAME stats\npredictions_norm = [0.5, NaN, -0.2, 0.8, NaN, 0.1]\npredictions_original = denormalize_labels(predictions_norm, stats)\nprintln(\"Predictions (original scale): \", predictions_original)","category":"page"},{"location":"examples/#The-Golden-Rule:-Stats-Based-Workflow","page":"Examples","title":"The Golden Rule: Stats-Based Workflow","text":"","category":"section"},{"location":"examples/#CORRECT:-Always-Use-This-Pattern","page":"Examples","title":"✅ CORRECT: Always Use This Pattern","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Step 1: Compute stats from training data ONLY\nstats = compute_normalization_stats(train_labels; method=:zscore, clip_quantiles=(0.01, 0.99))\n\n# Step 2: Apply SAME stats to all data splits\ntrain_norm = apply_normalization(train_labels, stats)\nval_norm = apply_normalization(val_labels, stats)    # Same stats\ntest_norm = apply_normalization(test_labels, stats)  # Same stats\n\n# Step 3: Denormalize predictions using SAME stats\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"examples/#WRONG:-Direct-Normalization-(Causes-Data-Leakage)","page":"Examples","title":"❌ WRONG: Direct Normalization (Causes Data Leakage)","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"# DON'T DO THIS - causes data leakage!\ntrain_norm = normalize_labels(train_labels)\ntest_norm = normalize_labels(test_labels)  # Different stats = data leakage!","category":"page"},{"location":"examples/#Why-Stats-Based-Workflow-is-Critical","page":"Examples","title":"Why Stats-Based Workflow is Critical","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Prevents Data Leakage: Test data never influences normalization parameters\nConsistent Scaling: All data splits use identical normalization\nProper Validation: Model performance reflects real-world generalization\nCorrect Predictions: Denormalization uses the same parameters as training","category":"page"},{"location":"examples/#Advanced-Examples","page":"Examples","title":"Advanced Examples","text":"","category":"section"},{"location":"examples/#Example-5:-Cross-Validation-with-Consistent-Stats","page":"Examples","title":"Example 5: Cross-Validation with Consistent Stats","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using RealLabelNormalization\nusing Statistics\n\n# Simulate a dataset for cross-validation\nn_samples = 1000\nX = randn(n_samples, 5)\ny = 2 * X[:, 1] + 0.5 * X[:, 2].^2 - X[:, 3] + 0.1 * randn(n_samples)\n\n# 5-fold cross-validation\nn_folds = 5\nfold_size = n_samples ÷ n_folds\n\nfor fold in 1:n_folds\n    println(\"\\\\n=== Fold $fold ===\")\n    \n    # Split data for this fold\n    val_start = (fold - 1) * fold_size + 1\n    val_end = fold * fold_size\n    val_idx = val_start:val_end\n    train_idx = setdiff(1:n_samples, val_idx)\n    \n    X_train, y_train = X[train_idx, :], y[train_idx]\n    X_val, y_val = X[val_idx, :], y[val_idx]\n    \n    # Step 1: Compute stats from training fold ONLY\n    stats = compute_normalization_stats(y_train; method=:zscore, clip_quantiles=(0.01, 0.99))\n    \n    # Step 2: Apply SAME stats to both training and validation\n    y_train_norm = apply_normalization(y_train, stats)\n    y_val_norm = apply_normalization(y_val, stats)  # Same stats!\n    \n    println(\"Training stats: mean=$(mean(y_train)), std=$(std(y_train))\")\n    println(\"Validation stats: mean=$(mean(y_val)), std=$(std(y_val))\")\n    println(\"Normalized training: mean=$(mean(y_train_norm)), std=$(std(y_train_norm))\")\n    println(\"Normalized validation: mean=$(mean(y_val_norm)), std=$(std(y_val_norm))\")\n    \n    # Step 3: Train model and make predictions\n    # model = train_model(X_train, y_train_norm)\n    # val_pred_norm = model(X_val)\n    # val_pred_original = denormalize_labels(val_pred_norm, stats)\nend","category":"page"},{"location":"examples/#Example-6:-Custom-Normalization-Ranges","page":"Examples","title":"Example 6: Custom Normalization Ranges","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using RealLabelNormalization\n\n# Original data\ndata = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n\nprintln(\"Original data: \", data)\n\n# Different target ranges\nranges = [(-1, 1), (0, 1), (-2, 2), (-10, 10)]\n\nfor range in ranges\n    normalized = normalize_labels(data; range=range, clip_quantiles=nothing)\n    actual_range = (minimum(normalized), maximum(normalized))\n    println(\"Target $range -> Actual $actual_range\")\nend","category":"page"},{"location":"examples/#Example-7:-Multi-Target-with-Different-Scales","page":"Examples","title":"Example 7: Multi-Target with Different Scales","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using RealLabelNormalization\n\n# Multi-target data with very different scales\n# Column 1: Small values (0-10)\n# Column 2: Medium values (100-1000)  \n# Column 3: Large values (10000-100000)\nmulti_scale_data = [1.0 100.0 10000.0;\n                    2.0 200.0 20000.0;\n                    3.0 300.0 30000.0;\n                    4.0 400.0 40000.0;\n                    5.0 500.0 50000.0;\n                    100.0 50000.0 5000.0]  # Outlier row\n\nprintln(\"Original data ranges per column:\")\nfor i in 1:3\n    col_range = [minimum(multi_scale_data[:, i]), maximum(multi_scale_data[:, i])]\n    println(\"  Column $i: $col_range\")\nend\n\n# Compare global vs column-wise normalization\nglobal_norm = normalize_labels(multi_scale_data; mode=:global)\ncolumn_norm = normalize_labels(multi_scale_data; mode=:columnwise)\n\nprintln(\"\\\\nGlobal normalization - range per column:\")\nfor i in 1:3\n    col_range = [minimum(global_norm[:, i]), maximum(global_norm[:, i])]\n    println(\"  Column $i: $col_range\")\nend\n\nprintln(\"\\\\nColumn-wise normalization - range per column:\")\nfor i in 1:3\n    col_range = [minimum(column_norm[:, i]), maximum(column_norm[:, i])]\n    println(\"  Column $i: $col_range\")\nend","category":"page"},{"location":"examples/#Performance-Considerations","page":"Examples","title":"Performance Considerations","text":"","category":"section"},{"location":"examples/#Example-8:-Large-Dataset-Handling","page":"Examples","title":"Example 8: Large Dataset Handling","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using RealLabelNormalization\nusing BenchmarkTools\n\n# Simulate large dataset\nlarge_data = randn(100_000, 10)  # 100k samples, 10 targets\n\nprintln(\"Dataset size: \", size(large_data))\n\n# Benchmark different operations\nprintln(\"\\\\nPerformance benchmarks:\")\n@btime normalize_labels($large_data; mode=:columnwise)\n@btime compute_normalization_stats($large_data; mode=:columnwise)\n@btime apply_normalization($large_data, $stats) setup=(stats=compute_normalization_stats($large_data; mode=:columnwise))","category":"page"},{"location":"#RealLabelNormalization.jl","page":"Home","title":"RealLabelNormalization.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package for robust normalization of real-valued labels, commonly used in regression tasks. This package provides various normalization methods with built-in outlier handling and NaN support.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Multiple normalization methods: Min-max and Z-score normalization\nFlexible normalization modes: Global or column-wise normalization\nRobust outlier handling: Configurable quantile-based clipping\nNaN handling: Preserves NaN values while computing statistics on valid data\nConsistent train/test normalization: Save statistics from training data and apply to test data","category":"page"},{"location":"#Quick-Start-(Stats-Based-Workflow)","page":"Home","title":"Quick Start (Stats-Based Workflow)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using RealLabelNormalization\n\n# Training labels with outlier\ntrain_labels = [1.5, 2.3, 4.1, 3.7, 5.2, 100.0]\ntest_labels = [2.1, 3.9, 4.5]\n\n# Step 1: Compute stats from TRAINING DATA ONLY\nstats = compute_normalization_stats(train_labels; method=:zscore, clip_quantiles=(0.01, 0.99))\n\n# Step 2: Apply SAME stats to training data\ntrain_normalized = apply_normalization(train_labels, stats)\n\n# Step 3: Apply SAME STATS to test data (prevents data leakage!)\ntest_normalized = apply_normalization(test_labels, stats)\n\n# Step 4: Train model on normalized data\n# model = train_your_model(X_train, train_normalized)\n\n# Step 5: Denormalize predictions back to original scale using SAME stats\npredictions_normalized = model(X_test)  # Model outputs normalized predictions\npredictions_original = denormalize_labels(predictions_normalized, stats)","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"RealLabelNormalization\")","category":"page"},{"location":"#API-Reference","page":"Home","title":"API Reference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#RealLabelNormalization.ColumnwiseScaleBack","page":"Home","title":"RealLabelNormalization.ColumnwiseScaleBack","text":"ColumnwiseScaleBack{T<:AbstractFloat, F, N}\n\nFunctor for denormalizing columnwise-normalized values. Uses a tuple of functors for GPU compatibility (bitstype).\n\nType Parameters\n\nT: Float type (Float32, Float64)\nF: Type of the per-column functor\nN: Number of columns (compile-time constant)\n\nFields\n\nfunctors::NTuple{N,F}: Tuple of functors, one per column\n\nExample\n\ncol_functors = (\n    MinMaxScaleBack{Float32}(0.0f0, 10.0f0, -1.0f0, 1.0f0),\n    ZScoreScaleBack{Float32}(5.0f0, 2.0f0)\n)\nfunctor = ColumnwiseScaleBack(col_functors)\noriginal_col1 = functor(0.0f0, 1)  # Denormalize for column 1\noriginal_col2 = functor(1.5f0, 2)  # Denormalize for column 2\n\nGPU Usage Note\n\nFor GPU kernels with large numbers of columns, consider using the raw parameter arrays (minvals, maxvals, etc.) directly from the stats object instead of the functor.\n\n\n\n\n\n","category":"type"},{"location":"#RealLabelNormalization.LogScaleBack","page":"Home","title":"RealLabelNormalization.LogScaleBack","text":"LogScaleBack{T<:AbstractFloat}\n\nFunctor for denormalizing log-transformed values back to original scale. Compatible with CUDA kernels - all fields are scalars (bitstype).\n\nFields\n\noffset::T: Offset added before log transformation\n\nExample\n\nfunctor = LogScaleBack{Float32}(1.0f0)\noriginal = functor(0.0f0)  # Returns 0.0f0 (exp(0) - 1)\n\n\n\n\n\n","category":"type"},{"location":"#RealLabelNormalization.MinMaxScaleBack","page":"Home","title":"RealLabelNormalization.MinMaxScaleBack","text":"MinMaxScaleBack{T<:AbstractFloat}\n\nFunctor for denormalizing min-max normalized values back to original scale. Compatible with CUDA kernels - all fields are scalars (bitstype).\n\nFields\n\nmin_val::T: Original minimum value\nmax_val::T: Original maximum value  \nrange_low::T: Lower bound of normalized range\nrange_high::T: Upper bound of normalized range\n\nExample\n\nfunctor = MinMaxScaleBack{Float32}(1.0f0, 5.0f0, -1.0f0, 1.0f0)\noriginal = functor(0.0f0)  # Returns 3.0f0 (midpoint)\n\n\n\n\n\n","category":"type"},{"location":"#RealLabelNormalization.RowwiseScaleBack","page":"Home","title":"RealLabelNormalization.RowwiseScaleBack","text":"RowwiseScaleBack{T<:AbstractFloat, F, N}\n\nFunctor for denormalizing rowwise-normalized values. Uses a tuple of functors for GPU compatibility (bitstype).\n\nType Parameters\n\nT: Float type (Float32, Float64)\nF: Type of the per-row functor\nN: Number of rows (compile-time constant)\n\nFields\n\nfunctors::NTuple{N,F}: Tuple of functors, one per row\n\nExample\n\nrow_functors = (\n    MinMaxScaleBack{Float32}(0.0f0, 10.0f0, -1.0f0, 1.0f0),\n    ZScoreScaleBack{Float32}(5.0f0, 2.0f0)\n)\nfunctor = RowwiseScaleBack(row_functors)\noriginal_row1 = functor(0.0f0, 1)  # Denormalize for row 1\noriginal_row2 = functor(1.5f0, 2)  # Denormalize for row 2\n\nGPU Usage Note\n\nFor GPU kernels with large numbers of rows, consider using the raw parameter arrays (minvals, maxvals, etc.) directly from the stats object instead of the functor. Tuples with many elements can increase compilation time.\n\n\n\n\n\n","category":"type"},{"location":"#RealLabelNormalization.ZScoreScaleBack","page":"Home","title":"RealLabelNormalization.ZScoreScaleBack","text":"ZScoreScaleBack{T<:AbstractFloat}\n\nFunctor for denormalizing z-score normalized values back to original scale. Compatible with CUDA kernels - all fields are scalars (bitstype).\n\nFields\n\nmean::T: Original mean value\nstd::T: Original standard deviation\n\nExample\n\nfunctor = ZScoreScaleBack{Float32}(3.0f0, 1.5f0)\noriginal = functor(0.0f0)  # Returns 3.0f0 (the mean)\n\n\n\n\n\n","category":"type"},{"location":"#RealLabelNormalization._apply_training_clip_bounds-Tuple{AbstractArray, NamedTuple}","page":"Home","title":"RealLabelNormalization._apply_training_clip_bounds","text":"Apply training clip bounds to validation/test data.\n\n\n\n\n\n","category":"method"},{"location":"#RealLabelNormalization._clip_outliers-Tuple{AbstractVector, Tuple{Real, Real}, Symbol}","page":"Home","title":"RealLabelNormalization._clip_outliers","text":"Clip outliers using quantiles before normalization.\n\n\n\n\n\n","category":"method"},{"location":"#RealLabelNormalization.apply_normalization-Tuple{AbstractArray, NamedTuple}","page":"Home","title":"RealLabelNormalization.apply_normalization","text":"apply_normalization(labels, stats)\n\nApply pre-computed normalization statistics to new data (validation/test sets).\n\nEnsures consistent normalization across train/validation/test splits using only training statistics. This includes applying the same clipping bounds if they were used during training.\n\n\n\n\n\n","category":"method"},{"location":"#RealLabelNormalization.compute_normalization_stats-Tuple{AbstractArray}","page":"Home","title":"RealLabelNormalization.compute_normalization_stats","text":"compute_normalization_stats(labels; method=:minmax, mode=:global, \nrange=(-1, 1), clip_quantiles=(0.01, 0.99), log_shift=100.0)\n\nCompute normalization statistics from training data for later application to validation/test sets.\n\nInputs\n\nlabels: Vector or matrix where the last dimension is the number of samples\nmethod::Symbol: Normalization method\n:minmax: Min-max normalization (default)\n:zscore: Z-score normalization (mean=0, std=1)\n:log: Log normalization (log-transform with automatic offset for non-positive values)\nrange::Tuple{Real,Real}: Target range for min-max normalization (default (-1, 1))\n(-1, 1): Scaled min-max to [-1,1] (default)\n(0, 1): Standard min-max to [0,1]\nCustom ranges: e.g., (-2, 2)\nNote: Ignored for :zscore and :log methods\nmode::Symbol: Normalization scope\n:global: Normalize across all values (default)\n:columnwise: Normalize each column independently\n:rowwise: Normalize each row independently\nclip_quantiles::Union{Nothing,Tuple{Real,Real}}: Percentile values (0-1) for outlier clipping before normalization\n(0.01, 0.99): Clip to 1st-99th percentiles (default)\n(0.05, 0.95): Clip to 5th-95th percentiles (more aggressive)\nnothing: No clipping\nlog_shift::Real: Shift parameter for log normalization (default: 100.0)\nFor :log method, the offset is computed as: offset = min_val <= 0 ? abs(min_val) + log_shift : 0.0\nLarger values make log normalization less sensitive to small values near zero\nOnly used when method=:log, ignored otherwise\n\nReturns\n\nNamed tuple with normalization parameters that can be used with apply_normalization\n\nExample\n\n# Compute stats from training data with outlier clipping\ntrain_stats = compute_normalization_stats(train_labels; method=:zscore, mode=:columnwise, clip_quantiles=(0.05, 0.95))\n\n# Apply to validation/test data (uses same clipping bounds)\nval_normalized = apply_normalization(val_labels, train_stats)\ntest_normalized = apply_normalization(test_labels, train_stats)\n\n# Log normalization for skewed distributions with custom shift\nlog_stats = compute_normalization_stats(train_labels; method=:log, log_shift=1000.0)\nval_log_normalized = apply_normalization(val_labels, log_stats)\n\n\n\n\n\n","category":"method"},{"location":"#RealLabelNormalization.denormalize_labels-Tuple{AbstractArray, NamedTuple}","page":"Home","title":"RealLabelNormalization.denormalize_labels","text":"denormalize_labels(normalized_labels, stats)\n\nConvert normalized labels back to original scale using stored statistics.\n\nUseful for interpreting model predictions in original units.\n\nUses the GPU-compatible functor stored in stats.scalebackfunctor.\n\n\n\n\n\n","category":"method"},{"location":"#RealLabelNormalization.normalize_labels-Tuple{AbstractArray}","page":"Home","title":"RealLabelNormalization.normalize_labels","text":"normalize_labels(labels; method=:minmax, range=(-1, 1), mode=:global, clip_quantiles=(0.01, 0.99), log_shift=100.0)\n\nNormalize labels with various normalization methods and modes. Handles NaN values by ignoring them  in statistical computations and preserving them in the output.\n\nArguments\n\nlabels: Vector or matrix where the last dimension is the number of samples\nmethod::Symbol: Normalization method\n:minmax: Min-max normalization (default)\n:zscore: Z-score normalization (mean=0, std=1)\n:log: Log normalization (log-transform with automatic offset for non-positive values)\nrange::Tuple{Real,Real}: Target range for min-max normalization (default: (-1, 1))\n(-1, 1): Scaled min-max to [-1,1] (default)\n(0, 1): Standard min-max to [0,1]\nCustom ranges: e.g., (-2, 2)\nNote: Ignored for :zscore and :log methods\nmode::Symbol: Normalization scope\n:global: Normalize across all values (default)\n:columnwise: Normalize each column independently\n:rowwise: Normalize each row independently\nclip_quantiles::Union{Nothing,Tuple{Real,Real}}: Percentile values (0-1) for outlier clipping before normalization\n(0.01, 0.99): Clip to 1st-99th percentiles (default)\n(0.05, 0.95): Clip to 5th-95th percentiles (more aggressive)\nnothing: No clipping\nlog_shift::Real: Shift parameter for log normalization (default: 100.0)\nFor :log method, the offset is computed as: offset = min_val <= 0 ? abs(min_val) + log_shift : 0.0\nLarger values make log normalization less sensitive to small values near zero\nOnly used when method=:log, ignored otherwise\n\nNaN Handling\n\nNaN values are ignored when computing statistics (min, max, mean, std, quantiles)\nNaN values are preserved in the output (remain as NaN)\nIf all values in a column are NaN, appropriate warnings are issued and NaN is returned\n\nReturns\n\nNormalized labels with same shape as input\n\nExamples\n\n# Vector labels (single target)\nlabels = [1.0, 5.0, 3.0, 8.0, 2.0, 100.0]  # 100.0 is outlier\n\n# Min-max to [-1,1] with outlier clipping (default)\nnormalized = normalize_labels(labels)\n\n# Min-max to [0,1] \nnormalized = normalize_labels(labels; range=(0, 1))\n\n# Z-score normalization with outlier clipping\nnormalized = normalize_labels(labels; method=:zscore)\n\n# Log normalization (useful for skewed distributions)\nnormalized = normalize_labels(labels; method=:log)\n\n# Log normalization with custom shift (less sensitive to small values)\nnormalized = normalize_labels(labels; method=:log, log_shift=1000.0)\n\n# Matrix labels (multi-target)\nlabels_matrix = [1.0 10.0; 5.0 20.0; 3.0 15.0; 8.0 25.0; 1000.0 5.0]  # Outlier in col 1\n\n# Global normalization with clipping\nnormalized = normalize_labels(labels_matrix; mode=:global)\n\n# Column-wise normalization with clipping \nnormalized = normalize_labels(labels_matrix; mode=:columnwise)\n\n# Row-wise normalization with clipping\nnormalized = normalize_labels(labels_matrix; mode=:rowwise)\n\n\n\n\n\n","category":"method"}]
}
